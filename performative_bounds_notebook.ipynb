{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Performative generalization bounds (Cor. 3.8-style)\n",
        "\n",
        "This notebook loads the attached run archive, extracts the trained model and predicted probabilities, and computes the bound of the form:\n",
        "\n",
        "$$\\mathscr R(d_0, \\widehat{\\theta}_T) \\leq \\mathscr R(\\widehat{d}_{T-1}, \\widehat{\\theta}_T)\n",
        "+ L_\\ell \\left(\\frac{\\log (C_a/\\delta)}{C_b\\,n}\\right)^{\\frac{p}{\\nu}}\n",
        "+ \\frac{(\\varepsilon^{T-1}-1)}{L_\\ell^{-1}(\\varepsilon-1)}\\,\\left(\\frac{m}{n}\\right)^{\\frac{1}{p}}\\,\\mathscr D_{\\mathcal Z}\\,\\tilde L_a$$\n",
        "\n",
        "with $\\tilde L_a = 1/(1+L_a)$, where $L_a$ is a (conservative) Lipschitz constant of the best-response map\n",
        "$$G(d)=\\arg\\min_{\\theta\\in\\Theta}\\mathscr R(d,\\theta).$$\n",
        "\n",
        "It also computes the special case $T=1$ (where the performative accumulation term vanishes).\n",
        "\n",
        "**Notes**\n",
        "- Some previously uploaded files in this chat may expire. If you get a file-not-found error, re-upload the archive.\n",
        "- Constants $C_a,C_b$ from Wasserstein concentration are not identified by the model; the notebook treats them as inputs (default 1).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, zipfile, glob, math, pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "ZIP_PATH = '/mnt/data/20260126_233204_pca_x2_y1.zip'  # update if needed\n",
        "EXTRACT_DIR = '/mnt/data/pca_x2_y1_extracted_nb'\n",
        "\n",
        "if not os.path.exists(ZIP_PATH):\n",
        "    raise FileNotFoundError(\n",
        "        f\"Could not find {ZIP_PATH}. If the upload expired, please re-upload the archive and update ZIP_PATH.\")\n",
        "\n",
        "if os.path.exists(EXTRACT_DIR):\n",
        "    import shutil\n",
        "    shutil.rmtree(EXTRACT_DIR)\n",
        "os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(ZIP_PATH, 'r') as z:\n",
        "    z.extractall(EXTRACT_DIR)\n",
        "\n",
        "train_paths = glob.glob(EXTRACT_DIR + '/**/train_predictions.csv', recursive=True)\n",
        "test_paths  = glob.glob(EXTRACT_DIR + '/**/test_predictions.csv', recursive=True)\n",
        "model_paths = glob.glob(EXTRACT_DIR + '/**/model.pkl', recursive=True)\n",
        "\n",
        "train_paths, test_paths, model_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pick the first matching set by default (adjust if your archive contains multiple runs)\n",
        "train_path = train_paths[0]\n",
        "test_path = test_paths[0] if test_paths else None\n",
        "model_path = model_paths[0] if model_paths else None\n",
        "\n",
        "df_train = pd.read_csv(train_path)\n",
        "n = len(df_train)\n",
        "\n",
        "eps = 1e-15\n",
        "p_hat = np.clip(df_train['y_pred_proba'].to_numpy(float), eps, 1-eps)\n",
        "y = df_train['y_true'].to_numpy(int)\n",
        "\n",
        "# Empirical logistic (cross-entropy) risk\n",
        "R_emp = float(-np.mean(y*np.log(p_hat) + (1-y)*np.log(1-p_hat)))\n",
        "\n",
        "R_emp, n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model (optional; used to report dimension or regularization metadata if present)\n",
        "clf = None\n",
        "d = None\n",
        "if model_path is not None and os.path.exists(model_path):\n",
        "    with open(model_path, 'rb') as f:\n",
        "        clf = pickle.load(f)\n",
        "    if hasattr(clf, 'coef_'):\n",
        "        d = clf.coef_.shape[1]\n",
        "\n",
        "d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parameters / constants\n",
        "\n",
        "Set the parameters used in the bound. Defaults follow the choices used in this thread:\n",
        "- Wasserstein order: $p=2$\n",
        "- PCA effective dimension: $\\nu=4$\n",
        "- $\\delta$ (confidence): default 0.1\n",
        "- $\\mathscr D_{\\mathcal Z}$: default 1 (after normalization)\n",
        "- $m/n$: default 0.03 (edit as needed)\n",
        "\n",
        "For $L_\\ell$ we use\n",
        "$$L_\\ell = \\frac{D_{\\mathcal X}}{1+\\exp(-D_{\\mathcal X}D_\\Theta)}$$\n",
        "with $D_{\\mathcal X}=\\mathrm{diam}([0,1]^4)=2$ and $D_\\Theta=\\mathrm{diam}([-3,3]^4)=12$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core parameters\n",
        "p_w = 2\n",
        "nu = 4\n",
        "delta = 0.1\n",
        "D_Z = 1.0\n",
        "\n",
        "# intervention share\n",
        "r = 0.03  # m/n\n",
        "\n",
        "# concentration constants (set by you / paper)\n",
        "C_a = 1.0\n",
        "C_b = 1.0\n",
        "\n",
        "# Lipschitz constant L_ell from diameters\n",
        "D_X = 2.0\n",
        "D_Theta = 12.0\n",
        "L_ell = D_X / (1.0 + math.exp(-D_X * D_Theta))\n",
        "\n",
        "L_ell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute $L_a$ and $\\tilde L_a$\n",
        "\n",
        "We use a conservative sensitivity bound for strongly convex ERM:\n",
        "$$\\|G(d)-G(d')\\| \\le \\frac{1}{\\mu}\\,\\sup_{\\theta}\\|\\nabla_\\theta \\mathscr R(d,\\theta)-\\nabla_\\theta \\mathscr R(d',\\theta)\\|,$$\n",
        "where $\\mu$ is the strong convexity modulus coming from $\\ell_2$ regularization.\n",
        "\n",
        "Under the common normalization\n",
        "$$\\mathscr R(d,\\theta)=\\mathbb E_d[\\ell(\\theta;z)] + \\tfrac{\\lambda}{2}\\|\\theta\\|^2,$$\n",
        "we have $\\mu=\\lambda$. For sklearn `LogisticRegression(C=...)`, a typical translation is $\\lambda\\approx 1/C$ (check your paperâ€™s convention).\n",
        "\n",
        "For logistic loss, a simple bound is $\\|\\nabla_\\theta \\ell(\\theta;(x,y))\\|\\le \\|x\\|\\le D_{\\mathcal X}$, so we take\n",
        "$$L_a \\approx \\frac{D_{\\mathcal X}}{\\lambda}, \\qquad \\tilde L_a=\\frac{1}{1+L_a}.$$\n",
        "\n",
        "Edit `lambda_reg` below to match your exact objective scaling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lambda_reg = 1.0  # set to your paper's lambda\n",
        "L_a = D_X / lambda_reg\n",
        "tilde_L_a = 1.0 / (1.0 + L_a)\n",
        "L_a, tilde_L_a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute the bound\n",
        "\n",
        "We compute:\n",
        "- empirical risk $\\mathscr R(\\widehat d_{T-1},\\widehat\\theta_T)$ (for $T=1$ this is the training empirical risk),\n",
        "- sampling term $L_\\ell \\big(\\log(C_a/\\delta)/(C_b n)\\big)^{p/\\nu}$,\n",
        "- performative accumulation term $\\frac{(\\varepsilon^{T-1}-1)}{L_\\ell^{-1}(\\varepsilon-1)} (m/n)^{1/p} \\mathscr D_{\\mathcal Z} \\tilde L_a$.\n",
        "\n",
        "The last term depends on $(\\varepsilon,T)$. For $T=1$ it is always zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sampling_term(L_ell, C_a, C_b, delta, n, p_w, nu):\n",
        "    return L_ell * ((math.log(C_a/delta)/(C_b*n)) ** (p_w/nu))\n",
        "\n",
        "def performative_accum_term(eps_val, T, L_ell, r, p_w, D_Z, tilde_L_a):\n",
        "    if T <= 1:\n",
        "        return 0.0\n",
        "    num = (eps_val**(T-1) - 1.0)\n",
        "    den = (1.0/L_ell) * (eps_val - 1.0)  # L_ell^{-1}(eps-1)\n",
        "    return (num/den) * (r ** (1.0/p_w)) * D_Z * tilde_L_a\n",
        "\n",
        "T = 1\n",
        "eps_val = 1.0 + L_a  # common choice; adjust if your paper uses a different eps\n",
        "\n",
        "S = sampling_term(L_ell, C_a, C_b, delta, n, p_w, nu)\n",
        "P = performative_accum_term(eps_val, T, L_ell, r, p_w, D_Z, tilde_L_a)\n",
        "BOUND = R_emp + S + P\n",
        "\n",
        "R_emp, S, P, BOUND"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: sweep over $r=m/n$ and $T$\n",
        "\n",
        "This is useful if you want to visualize how the performative accumulation term scales with $r$ and the number of performative steps $T$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_bound_for(r_val, T_val):\n",
        "    S = sampling_term(L_ell, C_a, C_b, delta, n, p_w, nu)\n",
        "    P = performative_accum_term(eps_val, T_val, L_ell, r_val, p_w, D_Z, tilde_L_a)\n",
        "    return R_emp + S + P, S, P\n",
        "\n",
        "r_grid = [0.01, 0.02, 0.03, 0.05]\n",
        "T_grid = [1, 2, 3, 5]\n",
        "\n",
        "rows = []\n",
        "for rr in r_grid:\n",
        "    for TT in T_grid:\n",
        "        b, s, pterm = compute_bound_for(rr, TT)\n",
        "        rows.append({'r=m/n': rr, 'T': TT, 'R_emp': R_emp, 'Sampling': s, 'PerformativeAccum': pterm, 'Bound': b})\n",
        "\n",
        "pd.DataFrame(rows)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}